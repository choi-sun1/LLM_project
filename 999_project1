from dotenv import load_dotenv
import os
import requests
import json
from pypdfloader import PyPDFLoader
from sentence_transformers import SentenceTransformer
import numpy as np
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import faiss
import re

# .env 파일 로드
load_dotenv()

# API 키 불러오기
API_KEY = os.getenv('API_KEY')

# 데이터 API 호출 및 저장
def fetch_data(api_url):
    headers = {'Authorization': f'Bearer {API_KEY}'}
    response = requests.get(api_url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        return None

# 예시로 데이터 호출
data = fetch_data('')

# JSON 파일로 저장
if data:
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# PDF에서 텍스트 추출
def extract_text_from_pdf(pdf_path):
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    return documents

# 텍스트 처리 및 청킹
def preprocess_and_chunk(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200, length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

# 텍스트 필터링 함수 (팀원 코드)
def preprocess_text(text):
    # 1. 처음에 숫자가 나오면 제거
    text = re.sub(r'^\s*\d+\s*', '', text)
    
    # 2. "비상시 국민행동요령 알아야 안전하다"로 시작하면 제거
    if text.startswith('비상시 국민행동요령 알아야 안전하다'):
        text = text[len('비상시 국민행동요령 알아야 안전하다'):].lstrip()
    
    # 3. 특정 패턴이 시작 부분에 있으면 제거 (패턴 1)
    pattern1 = r'^(\s*만화로 보는 비상시\s*국민행동요령\s*화생방 피해대비\s*행동요령\s*인명시설 피해시\s*행동요령\s*비상대비물자\s*준비 및 사용요령\s*비상사태시\s*행동요령\s*)'
    text = re.sub(pattern1, '', text)
    
    # 4. 특정 패턴이 시작 부분에 있으면 제거 (패턴 2)
    pattern2 = r'^(\s*온 가족이 함께\s*안전하게\s*화생방 피해대비\s*행동요령\s*인명시설 피해시\s*행동요령\s*비상대비물자\s*준비 및 사용요령\s*비상사태시\s*행동요령\s*화생방경보 발령시\s*국민행동요령\s*핵 경보 발령시\s*국민행동요령\s*)'
    text = re.sub(pattern2, '', text)
    
    # 5. 불필요한 줄 바꿈과 공백 정리
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    # 6. 특수 문자 제거 (URL에 사용되는 문자는 제외)
    text = re.sub(r'[^\w\sㄱ-힣:/\.\-]', '', text)
    
    # 처리 후 숫자가 나오면 제거
    text = re.sub(r'^\s*\d+\s*', '', text)
    return text

# 문서들에 전처리 적용
pdf_path = 'manual.pdf'
documents = extract_text_from_pdf(pdf_path)
for doc in documents:
    doc.page_content = preprocess_text(doc.page_content)

# 청킹
chunks = preprocess_and_chunk(documents)

# SentenceTransformer로 임베딩 생성
def create_embeddings(chunks):
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    embeddings = model.encode([chunk['page_content'] for chunk in chunks])
    return embeddings

embeddings = create_embeddings(chunks)

# FAISS로 벡터스토어 생성
def create_faiss_store(embeddings):
    index = faiss.IndexFlatL2(embeddings.shape[1])  # 벡터 차원 크기
    faiss.normalize_L2(embeddings)  # 벡터 정규화
    index.add(embeddings)
    return index

faiss_index = create_faiss_store(embeddings)

# 검색 기능 구현
def search(query, faiss_index, k=3):
    query_embedding = model.encode([query])
    faiss.normalize_L2(query_embedding)
    _, indices = faiss_index.search(query_embedding, k)
    return indices

# LLM 모델 호출 및 응답 생성
def generate_response(query, faiss_index, top_k=3):
    indices = search(query, faiss_index, top_k)
    relevant_chunks = [chunks[i] for i in indices[0]]
    
    # 검색된 문서로 OpenAI 모델에 질의
    openai_model = ChatOpenAI(model="gpt-4", temperature=0)
    context = "\n".join([chunk['page_content'] for chunk in relevant_chunks])
    response = openai_model.invoke([SystemMessage(content=context), HumanMessage(content=query)])
    
    return response.content

# 예시 질문에 대한 응답 생성
query = "화재 발생 시 대처 방법은?"
response = generate_response(query, faiss_index)
print(response)
